#!/usr/bin/env python3
# Copyright 2026 Canonical Ltd.
# See LICENSE file for licensing details.

"""
Storage Coordinator Module for Shared Storage Feature

This module provides coordination mechanisms for shared storage across multiple
Concourse CI Juju charm units, enabling efficient binary sharing and reducing
disk space usage from N×binary size to ~1.15×.

Key Components:
- SharedStorage: Represents the shared filesystem mounted across all units
- LockCoordinator: Manages exclusive locks for binary downloads using fcntl
- UpgradeState: Tracks upgrade coordination state via peer relation data
- WorkerDirectory: Per-unit isolated state on shared storage
- ServiceManager: Manages systemd service lifecycle during upgrades

Design Principles:
- File-based locking (POSIX fcntl) for simplicity
- Web/leader-only downloads with exclusive locks
- Workers poll for existing binaries (no downloads)
- Per-unit worker directories for isolation
- Backward compatible with non-shared deployments

Feature: 001-shared-storage
Author: Generated by /speckit.implement
"""

import fcntl
import json
import logging
import os
import subprocess
import time
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Generator, Iterator, Literal, Optional

logger = logging.getLogger(__name__)


# ============================================================================
# Exception Hierarchy
# ============================================================================


class SharedStorageError(Exception):
    """Base exception for shared storage operations."""

    pass


class StorageNotMountedError(SharedStorageError):
    """Shared storage volume not mounted."""

    pass


class LockAcquireError(SharedStorageError):
    """Failed to acquire exclusive lock."""

    pass


class StaleLockError(SharedStorageError):
    """Stale lock detected and cleaned."""

    pass


class ServiceManagementError(SharedStorageError):
    """Failed to manage systemd service."""

    pass


class UpgradeTimeoutError(SharedStorageError):
    """Upgrade coordination timeout."""

    pass


# ============================================================================
# Core Data Classes
# ============================================================================


@dataclass
class SharedStorage:
    """Shared storage volume configuration and state.

    Represents the shared filesystem mounted across all Concourse CI units,
    enabling efficient binary sharing and reducing disk space usage.

    Attributes:
        volume_path: Mount point for shared storage (e.g., /var/lib/concourse)
        filesystem_id: Unique filesystem identifier for validation
        installed_version: Current installed Concourse version (from marker file)
        bin_directory: Path to shared Concourse binaries
        keys_directory: Path to shared TSA keys
        lock_file_path: Path to .install.lock file for download coordination

    Raises:
        StorageNotMountedError: If volume_path doesn't exist or isn't accessible
    """

    volume_path: Path
    filesystem_id: str
    installed_version: Optional[str] = None
    bin_directory: Optional[Path] = None
    keys_directory: Optional[Path] = None
    lock_file_path: Optional[Path] = None

    def __post_init__(self):
        """Validate paths exist and are accessible."""
        logger.debug(f"Initializing SharedStorage at {self.volume_path}")
        if not self.volume_path.exists():
            raise StorageNotMountedError(f"Volume not mounted: {self.volume_path}")

        if not self.volume_path.is_dir():
            raise StorageNotMountedError(
                f"Volume path is not a directory: {self.volume_path}"
            )

        # Check accessibility (T063)
        try:
            # Check if we can access stats/permissions
            self.volume_path.stat()
        except OSError as e:
            raise StorageNotMountedError(f"Volume not accessible: {e}")

        # Set default subdirectories if not provided
        if self.bin_directory is None:
            self.bin_directory = self.volume_path / "bin"
        if self.keys_directory is None:
            self.keys_directory = self.volume_path / "keys"
        if self.lock_file_path is None:
            self.lock_file_path = self.volume_path / ".install.lock"

        # Ensure subdirectories exist
        self.bin_directory.mkdir(parents=True, exist_ok=True)
        self.keys_directory.mkdir(parents=True, exist_ok=True)

        # Enforce permissions (T075)
        # Binaries: rwxr-xr-x (755) - executable by all
        self.bin_directory.chmod(0o755)
        # Keys: rwx------ (700) - owner only
        self.keys_directory.chmod(0o700)

        # Read installed version if marker exists
        if self.installed_version is None:
            self.installed_version = self.read_installed_version()

    @property
    def version_marker_path(self) -> Path:
        """Path to .installed_version marker file."""
        return self.volume_path / ".installed_version"

    @property
    def progress_marker_path(self) -> Path:
        """Path to .download_in_progress marker file."""
        return self.volume_path / ".download_in_progress"

    @property
    def lxc_shared_marker_path(self) -> Path:
        """Path to .lxc_shared_storage marker file."""
        return self.volume_path / ".lxc_shared_storage"

    def is_lxc_shared_storage(self) -> bool:
        """Check if this is LXC-mounted shared storage.

        Returns:
            True if .lxc_shared_storage marker exists, False otherwise
        """
        return self.lxc_shared_marker_path.exists()

    def read_installed_version(self) -> Optional[str]:
        """Read installed version from marker file.

        Returns:
            Version string if marker exists, None otherwise
        """
        if not self.version_marker_path.exists():
            return None
        try:
            return self.version_marker_path.read_text().strip()
        except Exception as e:
            logger.warning(f"Failed to read installed version: {e}")
            return None

    def write_installed_version(self, version: str) -> None:
        """Write installed version to marker file (web/leader only).

        Uses atomic write via temp file + rename to prevent partial reads (T059).

        Args:
            version: Version string to write (e.g., "7.14.3")

        Note:
            This should only be called by web/leader units after
            successful binary download.
        """
        # Create temp file in same directory for atomic rename
        temp_file = self.version_marker_path.with_suffix(
            f".tmp.{os.getpid()}.{int(time.time())}"
        )
        logger.debug(f"Writing version {version} to temp file {temp_file}")
        try:
            temp_file.write_text(version)
            # Atomic rename (replace)
            os.replace(temp_file, self.version_marker_path)
            self.installed_version = version
            logger.info(f"Updated installed version marker to {version}")
        except Exception as e:
            if temp_file.exists():
                temp_file.unlink()
            raise SharedStorageError(f"Failed to write version marker: {e}")


@dataclass
class LockCoordinator:
    """Coordinates download locks via fcntl.

    Manages exclusive file locks for binary downloads to ensure only
    web/leader units download binaries while workers wait and reuse.

    Attributes:
        lock_path: Path to .install.lock file for fcntl locking
        holder_unit: Unit name currently holding the lock
        acquired_at: Timestamp when lock was acquired (UTC)
        timeout_seconds: Stale lock threshold in seconds (default: 10 minutes)

    Example:
        coordinator = LockCoordinator(lock_path=Path("/var/lib/concourse/.install.lock"))
        with coordinator.acquire_exclusive():
            # Download binaries here - protected by exclusive lock
            download_concourse_binaries()
    """

    lock_path: Path
    holder_unit: Optional[str] = None
    acquired_at: Optional[datetime] = None
    timeout_seconds: int = 600  # 10 minutes

    @contextmanager
    def acquire_exclusive(self, timeout: float = 0) -> Iterator[None]:
        """Acquire exclusive lock for binary download.

        Only web/leader should call this method. Uses fcntl.flock with
        LOCK_EX | LOCK_NB for non-blocking exclusive lock acquisition.

        Args:
            timeout: Maximum time to wait for lock (0 = non-blocking)

        Yields:
            None when lock successfully acquired

        Raises:
            LockAcquireError: If lock already held by another unit
            StaleLockError: If stale lock detected and cleaned

        Note:
            Lock is automatically released when context exits, even on exception.
        """
        start_time = time.time()
        logger.debug(
            f"Attempting to acquire lock at {self.lock_path} (timeout={timeout}s)"
        )

        while True:
            # Check for orphaned/stale locks first (T065)
            self.check_and_clean_orphaned_locks()

            # Ensure lock file parent directory exists
            self.lock_path.parent.mkdir(parents=True, exist_ok=True)

            lock_file = self.lock_path.open("w")
            try:
                # Non-blocking exclusive lock
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                self.holder_unit = os.environ.get("JUJU_UNIT_NAME", "unknown")
                self.acquired_at = datetime.now(timezone.utc)
                logger.info(
                    f"Lock acquired by {self.holder_unit} at {self.acquired_at}"
                )
                logger.debug(f"Holding lock on {self.lock_path}")
                try:
                    yield
                finally:
                    # Release lock
                    logger.debug(f"Releasing lock on {self.lock_path}")
                    try:
                        fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
                    except Exception as e:
                        logger.warning(f"Error releasing lock: {e}")
                    try:
                        lock_file.close()
                    except Exception:
                        pass
                    self.holder_unit = None
                    self.acquired_at = None
                return

            except BlockingIOError:
                lock_file.close()
                logger.debug("Lock held by another process")

                if timeout <= 0:
                    raise LockAcquireError(
                        "Download lock held by another unit (non-blocking). "
                        "Only one unit can download binaries at a time."
                    )

                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    raise LockAcquireError(
                        f"Timeout waiting for lock after {timeout}s. "
                        "Another unit is likely downloading binaries."
                    )

                time.sleep(1)
            self.holder_unit = None
            self.acquired_at = None

    def is_held(self) -> bool:
        """Check if lock is currently held by this instance.

        Returns:
            True if lock is held, False otherwise
        """
        return self.holder_unit is not None

    def check_and_clean_orphaned_locks(self) -> None:
        """Check for orphaned download markers and clean them (T065).

        If .download_in_progress exists but .install.lock is not held by any process,
        the marker is orphaned (crashed unit) and should be removed immediately.
        Also cleans up if marker is stale based on timeout.
        """
        progress_marker = self.lock_path.parent / ".download_in_progress"
        if not progress_marker.exists():
            return

        # Ensure directory exists
        self.lock_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            # We open for writing to check lock, but we don't truncate
            with self.lock_path.open("w") as lock_file:
                try:
                    # Try to acquire exclusive lock non-blocking
                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)

                    # If we got here, we acquired the lock, so no one else holds it.
                    # But marker exists. It is orphaned.
                    logger.warning(
                        "Found orphaned download marker (lock free), cleaning up"
                    )
                    if progress_marker.exists():
                        progress_marker.unlink()

                    # Release lock
                    fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)
                except BlockingIOError:
                    # Lock is held by someone. Check for staleness (timeout)
                    if self._is_stale():
                        logger.warning(
                            "Found stale download marker (timeout), cleaning up"
                        )
                        if progress_marker.exists():
                            progress_marker.unlink()
        except Exception as e:
            logger.warning(f"Error checking orphaned locks: {e}")

    def _is_stale(self) -> bool:
        """Check if progress marker is stale.

        Returns:
            True if .download_in_progress marker exists and is older
            than timeout_seconds, False otherwise
        """
        progress_marker = self.lock_path.parent / ".download_in_progress"
        if not progress_marker.exists():
            return False

        try:
            age_seconds = time.time() - progress_marker.stat().st_mtime
            is_stale = age_seconds > self.timeout_seconds
            if is_stale:
                logger.warning(
                    f"Progress marker is stale: {age_seconds:.0f}s old "
                    f"(threshold: {self.timeout_seconds}s)"
                )
            return is_stale
        except Exception as e:
            logger.error(f"Error checking progress marker age: {e}")
            return False

    def _clean_stale_markers(self) -> None:
        """Remove stale progress markers.

        Called when a stale .download_in_progress marker is detected,
        typically indicating a previous download failure or crash.
        """
        progress_marker = self.lock_path.parent / ".download_in_progress"
        try:
            if progress_marker.exists():
                progress_marker.unlink()
                logger.info("Cleaned stale progress marker")
        except Exception as e:
            logger.error(f"Failed to clean stale marker: {e}")


@dataclass
class UpgradeState:
    """Upgrade coordination state (stored in peer relation data).

    Tracks multi-unit upgrade progress via Juju peer relations. The leader
    transitions through states (idle → prepare → downloading → complete) while
    workers acknowledge readiness via worker_ready_count.

    Attributes:
        state: Current upgrade phase
        target_version: Version being upgraded to (e.g., "7.11.0")
        initiated_by: Unit name that initiated upgrade (e.g., "concourse-ci/0")
        timestamp: UTC timestamp of last state change
        worker_ready_count: Number of workers that stopped services
        expected_worker_count: Total workers expected to acknowledge
    """

    state: Literal["idle", "prepare", "downloading", "complete"]
    target_version: Optional[str]
    initiated_by: Optional[str]
    timestamp: datetime
    worker_ready_count: int = 0
    expected_worker_count: int = 0

    def to_relation_data(self) -> dict[str, str]:
        """Convert to peer relation data format.

        Returns:
            Dictionary with string keys/values for Juju relation storage
        """
        return {
            "upgrade-state": self.state,
            "target-version": self.target_version or "",
            "initiated-by": self.initiated_by or "",
            "timestamp": self.timestamp.isoformat(),
            "worker-ready-count": str(self.worker_ready_count),
            "expected-worker-count": str(self.expected_worker_count),
        }

    @classmethod
    def from_relation_data(cls, data: dict[str, str]) -> "UpgradeState":
        """Parse from peer relation data.

        Args:
            data: Relation data dictionary from Juju peer relation

        Returns:
            UpgradeState instance

        Raises:
            ValueError: If timestamp format is invalid
        """
        return cls(
            state=data.get("upgrade-state", "idle"),  # type: ignore
            target_version=data.get("target-version") or None,
            initiated_by=data.get("initiated-by") or None,
            timestamp=datetime.fromisoformat(data["timestamp"]),
            worker_ready_count=int(data.get("worker-ready-count", 0)),
            expected_worker_count=int(data.get("expected-worker-count", 0)),
        )

    def is_ready_to_download(self, timeout_seconds: int = 120) -> bool:
        """Check if all workers acknowledged (or timeout reached).

        Args:
            timeout_seconds: Maximum wait time for worker acknowledgment

        Returns:
            True if ready to proceed with download
        """
        if self.worker_ready_count >= self.expected_worker_count:
            return True

        elapsed = (datetime.now(timezone.utc) - self.timestamp).total_seconds()
        return elapsed >= timeout_seconds


@dataclass
class WorkerDirectory:
    """Per-worker isolated directory on shared storage.

    Each worker unit has an exclusive workspace under worker/{unit_name}/
    for its work_dir and state. This prevents contention while allowing
    shared read access to the common bin/ directory.

    Attributes:
        unit_name: Juju unit name (e.g., "concourse-ci/1")
        path: Full path to worker directory root
        state_file: Path to state.json for worker metadata
        work_dir: Concourse work directory for containers/volumes
    """

    unit_name: str
    path: Path
    state_file: Path
    work_dir: Path

    def __post_init__(self):
        """Ensure worker directory structure exists."""
        self.path.mkdir(parents=True, exist_ok=True)
        self.work_dir.mkdir(parents=True, exist_ok=True)

    @classmethod
    def from_shared_storage(
        cls, shared_storage: SharedStorage, unit_name: str
    ) -> "WorkerDirectory":
        """Create WorkerDirectory from shared storage root.

        Args:
            shared_storage: Parent shared storage instance
            unit_name: Juju unit name (e.g., "concourse-ci/1")

        Returns:
            WorkerDirectory instance with initialized paths
        """
        worker_path = shared_storage.volume_path / "worker" / unit_name
        return cls(
            unit_name=unit_name,
            path=worker_path,
            state_file=worker_path / "state.json",
            work_dir=worker_path / "work_dir",
        )

    def read_state(self) -> dict[str, Any]:
        """Read worker state from JSON file.

        Returns:
            State dictionary, empty dict if file doesn't exist
        """
        if not self.state_file.exists():
            return {}
        return json.loads(self.state_file.read_text())

    def write_state(self, state: dict[str, Any]) -> None:
        """Write worker state to JSON file.

        Args:
            state: State dictionary to persist
        """
        self.state_file.write_text(json.dumps(state, indent=2))


@dataclass
class ServiceManager:
    """Systemd service management for Concourse processes.

    Wraps systemctl operations with timeout protection and structured
    exception handling. Used during upgrades to stop workers before
    download and restart services after binary updates.

    Attributes:
        service_name: Systemd service name (e.g., "concourse-worker.service")
        timeout_seconds: Timeout for systemctl operations
    """

    service_name: str
    timeout_seconds: int = 30

    def stop(self) -> None:
        """Stop systemd service (workers before upgrade).

        Raises:
            ServiceManagementError: If stop fails or times out
        """
        try:
            subprocess.run(
                ["systemctl", "stop", self.service_name],
                check=True,
                capture_output=True,
                text=True,
                timeout=self.timeout_seconds,
            )
        except subprocess.CalledProcessError as e:
            raise ServiceManagementError(
                f"Failed to stop {self.service_name}: {e.stderr}"
            )
        except subprocess.TimeoutExpired:
            raise ServiceManagementError(
                f"Timeout stopping {self.service_name} after {self.timeout_seconds}s"
            )

    def start(self) -> None:
        """Start systemd service (workers after upgrade).

        Raises:
            ServiceManagementError: If start fails or times out
        """
        try:
            subprocess.run(
                ["systemctl", "start", self.service_name],
                check=True,
                capture_output=True,
                text=True,
                timeout=self.timeout_seconds,
            )
        except subprocess.CalledProcessError as e:
            raise ServiceManagementError(
                f"Failed to start {self.service_name}: {e.stderr}"
            )
        except subprocess.TimeoutExpired:
            raise ServiceManagementError(
                f"Timeout starting {self.service_name} after {self.timeout_seconds}s"
            )

    def restart(self) -> None:
        """Restart systemd service (web/leader after download).

        Raises:
            ServiceManagementError: If restart fails or times out
        """
        try:
            subprocess.run(
                ["systemctl", "restart", self.service_name],
                check=True,
                capture_output=True,
                text=True,
                timeout=self.timeout_seconds,
            )
        except subprocess.CalledProcessError as e:
            raise ServiceManagementError(
                f"Failed to restart {self.service_name}: {e.stderr}"
            )
        except subprocess.TimeoutExpired:
            raise ServiceManagementError(
                f"Timeout restarting {self.service_name} after {self.timeout_seconds}s"
            )


# ============================================================================
# Interface Implementations (Phase 2: T009-T011)
# ============================================================================


class StorageCoordinator:
    """Concrete implementation of storage coordination interfaces.

    Combines IStorageCoordinator, IProgressTracker, and IFilesystemValidator
    into a single coordinator class that manages shared storage operations.

    Attributes:
        storage: SharedStorage instance for this unit
        lock: LockCoordinator for download synchronization
        is_leader: Whether this unit is the leader (web/leader downloads only)
    """

    def __init__(self, storage: SharedStorage, lock: LockCoordinator, is_leader: bool):
        """Initialize storage coordinator.

        Args:
            storage: SharedStorage instance
            lock: LockCoordinator instance
            is_leader: True if this unit is leader/web
        """
        self.storage = storage
        self.lock = lock
        self._is_leader = is_leader
        self.logger = logging.getLogger(f"{__name__}.StorageCoordinator")

    # IStorageCoordinator implementation (T009)

    def is_web_leader(self) -> bool:
        """Check if current unit is web/leader."""
        return self._is_leader

    @contextmanager
    def acquire_download_lock(
        self, timeout_seconds: int = 0
    ) -> Generator[None, None, None]:
        """Acquire exclusive lock for binary download (web/leader only).

        Args:
            timeout_seconds: Maximum time to wait for lock (0 = non-blocking)

        Yields:
            None when lock acquired

        Raises:
            LockAcquireError: If lock held by another unit
            StaleLockError: If stale lock detected
            PermissionError: If caller is not web/leader
        """
        if not self.is_web_leader():
            raise PermissionError("Only web/leader units can download binaries")

        with self.lock.acquire_exclusive(timeout=timeout_seconds):
            yield

    def download_binaries(self, request: dict[str, Any]) -> dict[str, Any]:
        """Download and install Concourse binaries (web/leader only).

        Args:
            request: Download request with version, download_url, checksum_url, target_directory

        Returns:
            Result dictionary with success, version, installed_path, duration_seconds

        Raises:
            PermissionError: If caller is not web/leader
        """
        if not self.is_web_leader():
            raise PermissionError("Only web/leader units can download binaries")

        self.logger.debug(f"Starting binary download for v{request.get('version')}")

        if not self.lock.is_held():
            raise LockAcquireError("Download lock must be acquired before downloading")

        start_time = time.time()
        version = request["version"]

        try:
            self.mark_download_started(version)

            # TODO: Actual download/extraction logic in Phase 3
            # For now, just mark as complete
            self.logger.info(f"Download binaries v{version} (stub implementation)")

            self.mark_download_complete(version)

            return {
                "success": True,
                "version": version,
                "installed_path": str(self.storage.bin_directory),
                "duration_seconds": time.time() - start_time,
                "error_message": None,
            }
        except Exception as e:
            self.logger.error(f"Download failed: {e}")
            return {
                "success": False,
                "version": version,
                "installed_path": str(self.storage.bin_directory),
                "duration_seconds": time.time() - start_time,
                "error_message": str(e),
            }

    def get_installed_version(self) -> Optional[str]:
        """Read currently installed version from marker file."""
        return self.storage.read_installed_version()

    def wait_for_binaries(
        self,
        expected_version: str,
        timeout_seconds: int = 300,
        poll_interval_seconds: int = 5,
    ) -> bool:
        """Wait for binaries to be available (worker units).

        Args:
            expected_version: Version to wait for
            timeout_seconds: Maximum time to wait (default: 5 minutes, 0=infinite)
            poll_interval_seconds: Initial polling interval (default: 5 seconds)

        Returns:
            True if binaries available, False on timeout
        """
        start_time = time.time()
        timeout_desc = f"{timeout_seconds}s" if timeout_seconds > 0 else "infinite"
        self.logger.info(
            f"Waiting for binaries v{expected_version} (timeout: {timeout_desc})"
        )

        current_interval = poll_interval_seconds

        while True:
            installed_version = self.get_installed_version()
            if installed_version == expected_version:
                elapsed = time.time() - start_time
                self.logger.info(f"Binaries ready after {elapsed:.1f}s")
                return True

            elapsed = time.time() - start_time
            if timeout_seconds > 0 and elapsed >= timeout_seconds:
                self.logger.warning(
                    f"Timeout waiting for binaries after {elapsed:.1f}s "
                    f"(expected: {expected_version}, found: {installed_version})"
                )
                return False

            self.logger.debug(f"Binaries not ready, sleeping {current_interval}s...")
            time.sleep(current_interval)
            # Exponential backoff: 5 -> 10 -> 20 (cap at 20s)
            current_interval = min(current_interval * 2, 20)

    def verify_binaries(self, version: str) -> bool:
        """Verify that binaries for given version are valid.

        Args:
            version: Version to verify

        Returns:
            True if binaries valid, False otherwise
        """
        installed_version = self.get_installed_version()
        if installed_version != version:
            self.logger.warning(
                f"Version mismatch: expected {version}, found {installed_version}"
            )
            return False

        # Check for essential binaries
        required_binaries = ["concourse"]
        for binary in required_binaries:
            binary_path = self.storage.bin_directory / binary
            if not binary_path.exists():
                self.logger.warning(f"Missing binary: {binary_path}")
                return False

            if not os.access(binary_path, os.X_OK):
                self.logger.warning(f"Binary not executable: {binary_path}")
                return False

        # Verify checksum if available (T064)
        checksum_file = self.storage.bin_directory / ".concourse.sha256"
        concourse_bin = self.storage.bin_directory / "concourse"

        if checksum_file.exists() and concourse_bin.exists():
            try:
                import hashlib

                expected_sha256 = checksum_file.read_text().strip()

                hasher = hashlib.sha256()
                with open(concourse_bin, "rb") as f:
                    while chunk := f.read(8192):
                        hasher.update(chunk)
                actual_sha256 = hasher.hexdigest()

                if actual_sha256 != expected_sha256:
                    self.logger.error(
                        f"Binary corruption detected! "
                        f"Expected {expected_sha256}, got {actual_sha256}"
                    )
                    return False
                self.logger.info("Binary checksum verified successfully")
            except Exception as e:
                self.logger.warning(f"Failed to verify binary checksum: {e}")
                # Don't fail if just verification error (missing file etc), but if corruption...
                # We already logged error above if mismatch.

        return True

    def create_worker_directory(self, unit_name: str) -> Path:
        """Create isolated worker directory on shared storage.

        Args:
            unit_name: Juju unit name (e.g., "concourse-ci/1")

        Returns:
            Path to worker directory
        """
        worker_dir = WorkerDirectory.from_shared_storage(self.storage, unit_name)
        self.logger.info(f"Created worker directory: {worker_dir.path}")
        return worker_dir.path

    # IProgressTracker implementation (T010)

    def mark_download_started(self, version: str) -> None:
        """Create progress marker file (web/leader only).

        Args:
            version: Version being downloaded

        Raises:
            LockAcquireError: If download marker already exists (concurrent attempt or recent crash)
        """
        # Ensure bin_directory exists before creating marker
        self.storage.bin_directory.mkdir(parents=True, exist_ok=True)
        progress_marker = self.storage.bin_directory / ".download_in_progress"

        # Check for concurrent download (T055)
        if progress_marker.exists():
            # If marker exists and we are here (holding lock), it means:
            # 1. Another unit is downloading (but we have lock?)
            # 2. Previous download crashed recently (marker not stale yet)
            # In either case, we shouldn't proceed to avoid corruption
            try:
                content = progress_marker.read_text().splitlines()
                existing_version = content[0] if content else "unknown"
                timestamp = content[1] if len(content) > 1 else "unknown"

                raise LockAcquireError(
                    f"Download already in progress for v{existing_version} "
                    f"(started at {timestamp}). If this is a stale lock from a "
                    f"crashed unit, it will expire in 10 minutes."
                )
            except (OSError, IndexError):
                # Handle unreadable/empty marker
                raise LockAcquireError(
                    "Download marker exists but is unreadable. "
                    "Waiting for stale lock cleanup."
                )

        # Atomic write (T059)
        temp_file = progress_marker.with_suffix(
            f".tmp.{os.getpid()}.{int(time.time())}"
        )
        try:
            temp_file.write_text(f"{version}\n{datetime.now(timezone.utc).isoformat()}")
            os.replace(temp_file, progress_marker)
            self.logger.info(f"Marked download started for v{version}")
        except Exception as e:
            if temp_file.exists():
                temp_file.unlink()
            raise SharedStorageError(f"Failed to create progress marker: {e}")

    def mark_download_complete(self, version: str) -> None:
        """Remove progress marker, write version marker (web/leader only).

        Args:
            version: Version successfully downloaded
        """
        progress_marker = self.storage.bin_directory / ".download_in_progress"
        try:
            if progress_marker.exists():
                progress_marker.unlink()
        except Exception as e:
            self.logger.warning(f"Failed to remove progress marker: {e}")

        self.storage.write_installed_version(version)
        self.logger.info(f"Marked download complete for v{version}")

    def is_download_in_progress(self) -> bool:
        """Check if download currently in progress."""
        progress_marker = self.storage.bin_directory / ".download_in_progress"
        return progress_marker.exists()

    def get_download_age_seconds(self) -> Optional[float]:
        """Get age of current download in progress.

        Returns:
            Seconds since download started, or None if no download in progress
        """
        progress_marker = self.storage.bin_directory / ".download_in_progress"
        if not progress_marker.exists():
            return None

        try:
            return time.time() - progress_marker.stat().st_mtime
        except Exception as e:
            self.logger.error(f"Error checking progress marker age: {e}")
            return None

    # IFilesystemValidator implementation (T011)

    def get_filesystem_id(self, path: Path) -> str:
        """Get unique filesystem identifier for given path.

        Args:
            path: Path to check

        Returns:
            Filesystem ID (device:inode format)
        """
        stat_info = path.stat()
        return f"{stat_info.st_dev}:{stat_info.st_ino}"

    def validate_shared_mount(self, path: Path, expected_fs_id: str) -> bool:
        """Validate that path is on expected shared filesystem.

        Args:
            path: Path to validate
            expected_fs_id: Expected filesystem ID

        Returns:
            True if filesystem matches, False otherwise
        """
        actual_fs_id = self.get_filesystem_id(path)
        matches = actual_fs_id == expected_fs_id

        if not matches:
            self.logger.warning(
                f"Filesystem ID mismatch: expected {expected_fs_id}, "
                f"found {actual_fs_id}"
            )

        return matches

    def is_writable(self, path: Path) -> bool:
        """Check if path is writable by current unit.

        Performs an actual write test to ensure filesystem is mounted read-write
        and permissions are correct (T057).

        Args:
            path: Path to check

        Returns:
            True if writable, False otherwise
        """
        if not path.exists():
            return False

        # First check permission bits
        if not os.access(path, os.W_OK):
            return False

        # Then try actual write to catch Read-only filesystem errors
        try:
            test_file = path / f".write_test_{os.getpid()}_{int(time.time())}"
            self.logger.debug(f"Testing write access to {path} via {test_file}")
            test_file.touch()
            test_file.unlink()
            self.logger.debug(f"Write test passed for {path}")
            return True
        except (OSError, PermissionError) as e:
            self.logger.warning(f"Write test failed for {path}: {e}")
            return False


# ============================================================================
# Upgrade Coordination (Phase 2: T012-T014 - Stub implementations)
# ============================================================================
# Note: Full implementations will be completed during Phase 4 (User Story 2)
# These stubs provide the interface structure needed for Phase 3


class UpgradeCoordinator:
    """Coordinator for multi-unit upgrades via peer relations.

    Coordinates rolling upgrades across multiple Concourse units using Juju peer
    relations. Leader orchestrates the upgrade, workers respond to signals.

    Attributes:
        storage_coordinator: StorageCoordinator instance
        service_manager: ServiceManager instance
        relation_accessor: RelationDataAccessor for peer relation data
        is_leader: Whether this unit is leader
    """

    def __init__(
        self,
        storage_coordinator: StorageCoordinator,
        service_manager: ServiceManager,
        relation_accessor: "RelationDataAccessor",
        is_leader: bool,
    ):
        self.storage = storage_coordinator
        self.service_mgr = service_manager
        self.relation = relation_accessor
        self._is_leader = is_leader
        self.logger = logging.getLogger(f"{__name__}.UpgradeCoordinator")

    def initiate_upgrade(self, target_version: str, expected_workers: int = 0) -> None:
        """Initiate upgrade process (web/leader only).

        Sets phase=PREPARE in peer relation data and triggers relation-changed
        on all units.

        Args:
            target_version: Version to upgrade to (e.g., "7.14.3")
            expected_workers: Number of workers expected to acknowledge

        Raises:
            PermissionError: If caller is not web/leader
        """
        if not self._is_leader:
            raise PermissionError("Only leader can initiate upgrade")

        # Get current state and verify no upgrade in progress
        current_state = self.get_upgrade_state()
        if current_state and current_state.state != "idle":
            raise SharedStorageError(
                f"Upgrade already in progress (state={current_state.state})"
            )

        unit_name = os.environ.get("JUJU_UNIT_NAME", "unknown")

        upgrade_state = UpgradeState(
            state="prepare",
            target_version=target_version,
            initiated_by=unit_name,
            timestamp=datetime.now(timezone.utc),
            worker_ready_count=0,
            expected_worker_count=expected_workers,
        )

        # Write state to peer relation
        self.logger.debug(f"Writing upgrade state: {upgrade_state}")
        for key, value in upgrade_state.to_relation_data().items():
            self.relation.set_app_data(key, value)

        self.logger.info(
            f"Upgrade to v{target_version} initiated by {unit_name}, "
            f"expecting {expected_workers} workers"
        )

    def wait_for_workers_ready(self, timeout_seconds: int = 120) -> bool:
        """Wait for all workers to acknowledge prepare signal (web/leader only).

        Polls peer relation data for worker acknowledgments with exponential backoff.

        Args:
            timeout_seconds: Maximum time to wait (default: 2 minutes)

        Returns:
            True if all workers ready, False on timeout

        Raises:
            PermissionError: If caller is not web/leader
            UpgradeTimeoutError: If workers don't acknowledge within timeout
        """
        if not self._is_leader:
            raise PermissionError("Only leader can wait for workers")

        start_time = time.time()
        poll_interval = 2  # Start with 2-second intervals

        while (time.time() - start_time) < timeout_seconds:
            state = self.get_upgrade_state()
            if not state:
                raise SharedStorageError("Upgrade state lost during wait")

            # Count workers who have set upgrade-ready=true
            ready_count = 0
            if hasattr(self.relation, "relation") and self.relation.relation:
                for unit in self.relation.relation.units:
                    # Check if unit has set ready flag (use fresh data)
                    val = self.relation.get_fresh_unit_data(unit.name, "upgrade-ready")
                    if val == "true":
                        ready_count += 1
            else:
                # Fallback for testing
                ready_count = state.worker_ready_count

            expected_count = state.expected_worker_count

            self.logger.debug(
                f"Workers ready: {ready_count}/{expected_count} "
                f"(elapsed: {int(time.time() - start_time)}s)"
            )

            if ready_count >= expected_count:
                self.logger.info(f"All {expected_count} workers ready")
                return True

            time.sleep(poll_interval)
            poll_interval = min(poll_interval * 1.5, 10)  # Max 10s intervals

        # Timeout reached
        state = self.get_upgrade_state()
        ready_count = state.worker_ready_count if state else 0
        expected_count = state.expected_worker_count if state else 0

        self.logger.warning(
            f"Timeout waiting for workers: {ready_count}/{expected_count} ready "
            f"after {timeout_seconds}s"
        )

        raise UpgradeTimeoutError(
            f"Only {ready_count}/{expected_count} workers acknowledged after "
            f"{timeout_seconds}s timeout"
        )

    def mark_download_phase(self) -> None:
        """Set phase=DOWNLOADING in peer relation (web/leader only).

        Called before starting binary download to signal workers that
        download is in progress.

        Raises:
            PermissionError: If caller is not web/leader
        """
        if not self._is_leader:
            raise PermissionError("Only leader can mark download phase")

        state = self.get_upgrade_state()
        if not state:
            raise SharedStorageError("No upgrade in progress")

        state.state = "downloading"
        state.timestamp = datetime.now(timezone.utc)

        for key, value in state.to_relation_data().items():
            self.relation.set_app_data(key, value)

        self.logger.info("Marked upgrade phase: DOWNLOADING")

    def complete_upgrade(self) -> None:
        """Set phase=COMPLETE in peer relation (web/leader only).

        Called after binaries downloaded and web/leader service restarted.
        Workers will detect this signal and restart their services.

        Raises:
            PermissionError: If caller is not web/leader
        """
        if not self._is_leader:
            raise PermissionError("Only leader can complete upgrade")

        state = self.get_upgrade_state()
        if not state:
            raise SharedStorageError("No upgrade in progress")

        state.state = "complete"
        state.timestamp = datetime.now(timezone.utc)

        for key, value in state.to_relation_data().items():
            self.relation.set_app_data(key, value)

        self.logger.info(
            f"Upgrade to v{state.target_version} completed, " f"workers can now restart"
        )

    def reset_upgrade_state(self) -> None:
        """Reset to phase=IDLE (web/leader only).

        Called after all workers have restarted and upgrade is fully complete.

        Raises:
            PermissionError: If caller is not web/leader
        """
        if not self._is_leader:
            raise PermissionError("Only leader can reset upgrade state")

        idle_state = UpgradeState(
            state="idle",
            target_version=None,
            initiated_by=None,
            timestamp=datetime.now(timezone.utc),
            worker_ready_count=0,
            expected_worker_count=0,
        )

        for key, value in idle_state.to_relation_data().items():
            self.relation.set_app_data(key, value)

        self.logger.info("Upgrade state reset to IDLE")

    def get_upgrade_state(self) -> Optional[UpgradeState]:
        """Read current upgrade state from peer relation.

        Safe to call from any unit (leader or worker).

        Returns:
            Current upgrade state, or None if no state exists
        """
        try:
            data = self.relation.get_app_data()
            if not data or "upgrade-state" not in data:
                return None

            # Handle missing timestamp
            if "timestamp" not in data:
                data["timestamp"] = datetime.now(timezone.utc).isoformat()

            return UpgradeState.from_relation_data(data)
        except (KeyError, ValueError) as e:
            self.logger.warning(f"Failed to parse upgrade state: {e}")
            return None

    def handle_prepare_signal(self) -> None:
        """Handle PREPARE signal from web/leader (worker units only).

        Steps:
        1. Stop concourse-worker.service
        2. Increment worker_ready_count in peer relation
        3. Wait for COMPLETE signal

        Raises:
            PermissionError: If caller is web/leader
            ServiceManagementError: If service stop fails
        """
        if self._is_leader:
            raise PermissionError("Leader cannot handle prepare signal")

        state = self.get_upgrade_state()
        if not state or state.state != "prepare":
            self.logger.warning("No PREPARE signal detected")
            return

        self.logger.info(
            f"Handling PREPARE signal for upgrade to v{state.target_version}"
        )

        # Stop worker service
        try:
            self.logger.debug(f"Stopping worker service via {self.service_mgr}")
            self.service_mgr.stop()
            self.logger.info("Worker service stopped successfully")
        except ServiceManagementError as e:
            self.logger.error(f"Failed to stop worker service: {e}")
            raise

        # Increment ready count
        # state.worker_ready_count += 1  # Cannot update global count from worker
        state.timestamp = datetime.now(timezone.utc)

        # for key, value in state.to_relation_data().items():
        #     self.relation.set_app_data(key, value)

        # Set unit-specific flag
        self.relation.set_unit_data("upgrade-ready", "true")
        self.relation.set_unit_data(
            "upgrade-ready-timestamp", datetime.now(timezone.utc).isoformat()
        )

        self.logger.info(f"Acknowledged PREPARE signal (set upgrade-ready=true)")

    def handle_complete_signal(self) -> None:
        """Handle COMPLETE signal from web/leader (worker units only).

        Steps:
        1. Verify new binaries are installed
        2. Start concourse-worker.service
        3. Clear upgrade-ready flag

        Raises:
            PermissionError: If caller is web/leader
            ServiceManagementError: If service start fails
            SharedStorageError: If new binaries invalid
        """
        if self._is_leader:
            raise PermissionError("Leader cannot handle complete signal")

        state = self.get_upgrade_state()
        if not state or state.state != "complete":
            self.logger.warning("No COMPLETE signal detected")
            return

        unit_name = os.environ.get("JUJU_UNIT_NAME", "unknown")

        self.logger.info(
            f"Handling COMPLETE signal for upgrade to v{state.target_version}"
        )

        # Verify new binaries
        self.logger.debug(f"Verifying binaries for v{state.target_version}")
        if not self.storage.verify_binaries(state.target_version):
            raise SharedStorageError(
                f"Binary verification failed for v{state.target_version}"
            )

        self.logger.info("New binaries verified successfully")

        # Start worker service
        try:
            self.service_mgr.start()
            self.logger.info("Worker service started successfully")
        except ServiceManagementError as e:
            self.logger.error(f"Failed to start worker service: {e}")
            raise

        # Clear unit flags
        self.relation.set_unit_data("upgrade-ready", "false")

        self.logger.info(f"Upgrade to v{state.target_version} completed on {unit_name}")


class SystemdServiceManager:
    """Wrapper for systemd operations with enhanced features.

    Extends ServiceManager with status checking and validation.
    """

    def __init__(self, service_name: str, timeout_seconds: int = 30):
        self.service = ServiceManager(service_name, timeout_seconds)
        self.logger = logging.getLogger(f"{__name__}.SystemdServiceManager")

    def stop(self) -> None:
        """Stop systemd service."""
        self.service.stop()

    def start(self) -> None:
        """Start systemd service."""
        self.service.start()

    def restart(self) -> None:
        """Restart systemd service."""
        self.service.restart()

    def is_active(self) -> bool:
        """Check if service is currently active.

        Returns:
            True if service active, False otherwise
        """
        try:
            result = subprocess.run(
                ["systemctl", "is-active", self.service.service_name],
                capture_output=True,
                text=True,
                timeout=5,
            )
            return result.returncode == 0 and result.stdout.strip() == "active"
        except Exception as e:
            self.logger.warning(f"Error checking service status: {e}")
            return False


class RelationDataAccessor:
    """Accessor for Juju peer relation data.

    Provides interface to read/write peer relation data for upgrade coordination.
    Can work with ops.Relation objects or fallback to mock storage for testing.

    Attributes:
        relation: Optional ops.Relation object
        relation_name: Name of peer relation (default: "peers")
    """

    def __init__(self, relation: Optional[Any] = None, relation_name: str = "peers"):
        self.relation = relation
        self.relation_name = relation_name
        self.logger = logging.getLogger(f"{__name__}.RelationDataAccessor")
        self._mock_app_data: dict[str, str] = {}  # Fallback for testing
        self._mock_unit_data: dict[str, dict[str, str]] = {}  # Fallback for testing

    def set_app_data(self, key: str, value: str) -> None:
        """Set application-level data in peer relation.

        Args:
            key: Data key
            value: Data value (must be string)
        """
        if self.relation and hasattr(self.relation, "data"):
            try:
                # Try to use ops.Relation API
                self.relation.data[self.relation.app][key] = value

                # Force immediate update via CLI to avoid deadlock in blocking actions
                # ops framework commits changes at hook end, but we need workers
                # to see this while we wait in the action
                import subprocess

                cmd = [
                    "relation-set",
                    "-r",
                    str(self.relation.id),
                    f"{key}={value}",
                    "--app",
                ]
                subprocess.run(cmd, check=True)

            except Exception as e:
                self.logger.warning(f"Failed to set app data via relation: {e}")
                self._mock_app_data[key] = value
        else:
            self._mock_app_data[key] = value

    def get_app_data(self) -> dict[str, str]:
        """Get all application-level data from peer relation.

        Returns:
            Dictionary of all app-level key-value pairs
        """
        if self.relation and hasattr(self.relation, "data"):
            try:
                # Try to use ops.Relation API
                app_data = dict(self.relation.data[self.relation.app])
                return app_data
            except Exception as e:
                self.logger.warning(f"Failed to get app data via relation: {e}")
                return dict(self._mock_app_data)
        else:
            return dict(self._mock_app_data)

    def set_unit_data(self, key: str, value: str) -> None:
        """Set data on current unit.

        Args:
            key: Data key
            value: Data value (must be string)
        """
        unit_name = os.environ.get("JUJU_UNIT_NAME", "local")

        if self.relation and hasattr(self.relation, "data"):
            try:
                # Try to use ops.Relation API
                local_unit = None
                for unit in self.relation.units:
                    if unit.name == unit_name:
                        local_unit = unit
                        break

                if local_unit:
                    self.relation.data[local_unit][key] = value
                else:
                    self._mock_unit_data.setdefault(unit_name, {})[key] = value
            except Exception as e:
                self.logger.warning(f"Failed to set unit data via relation: {e}")
                self._mock_unit_data.setdefault(unit_name, {})[key] = value
        else:
            self._mock_unit_data.setdefault(unit_name, {})[key] = value

    def get_unit_data(self, unit_name: str, key: str) -> Optional[str]:
        """Get data from specific unit.

        Args:
            unit_name: Name of unit to query
            key: Data key

        Returns:
            Value if exists, None otherwise
        """
        if self.relation and hasattr(self.relation, "data"):
            try:
                for unit in self.relation.units:
                    if unit.name == unit_name:
                        return self.relation.data[unit].get(key)
            except Exception as e:
                self.logger.warning(f"Failed to get unit data via relation: {e}")

        return self._mock_unit_data.get(unit_name, {}).get(key)

    def get_fresh_unit_data(self, unit_name: str, key: str) -> Optional[str]:
        """Get fresh data from specific unit using CLI tools (bypassing ops cache).

        Necessary when waiting for updates within a single hook execution.
        """
        if self.relation and hasattr(self.relation, "id"):
            import subprocess

            try:
                # relation-get <key> <unit> -r <id>
                cmd = ["relation-get", "-r", str(self.relation.id), key, unit_name]
                self.logger.debug(f"Executing: {' '.join(cmd)}")
                # Use --app is false (default)
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    val = result.stdout.strip()
                    self.logger.debug(f"relation-get result: {val}")
                    return val if val else None
                else:
                    self.logger.warning(f"relation-get failed: {result.stderr}")
            except Exception as e:
                self.logger.warning(f"Failed to get fresh unit data: {e}")

        # Fallback to cached data
        return self.get_unit_data(unit_name, key)

    def get_all_units(self) -> list[str]:
        """Get list of all units in relation.

        Returns:
            List of unit names
        """
        if self.relation and hasattr(self.relation, "units"):
            try:
                return [unit.name for unit in self.relation.units]
            except Exception as e:
                self.logger.warning(f"Failed to get units via relation: {e}")

        return list(self._mock_unit_data.keys())


# ============================================================================
# Module Initialization
# ============================================================================

__all__ = [
    # Exceptions
    "SharedStorageError",
    "StorageNotMountedError",
    "LockAcquireError",
    "StaleLockError",
    "ServiceManagementError",
    "UpgradeTimeoutError",
    # Data classes
    "SharedStorage",
    "LockCoordinator",
    "UpgradeState",
    "WorkerDirectory",
    "ServiceManager",
    # Coordinators
    "StorageCoordinator",
    "UpgradeCoordinator",
    "SystemdServiceManager",
    "RelationDataAccessor",
]
