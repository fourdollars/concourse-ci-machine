<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Learn how to automatically mount datasets into your Concourse CI GPU tasks - A complete tutorial with examples">
    <title>Mounting Datasets for ML Tasks - Concourse CI Machine Charm</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <style>
        :root {
            --primary-color: #5c3c92;
            --secondary-color: #11a8cd;
            --accent-color: #fd5f00;
            --success-color: #0e8420;
            --warning-color: #f99b11;
            --text-dark: #111;
            --text-light: #666;
            --bg-light: #f7f7f7;
            --bg-white: #fff;
            --border-color: #d9d9d9;
            --tutorial-color: #5e2750;
            --howto-color: #206a5d;
            --reference-color: #216583;
            --explanation-color: #852c2b;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Ubuntu', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Cantarell, sans-serif;
            line-height: 1.6;
            color: var(--text-dark);
            background-color: var(--bg-white);
        }

        header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 2rem;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 300;
        }

        .breadcrumbs {
            background: var(--bg-light);
            padding: 1rem 2rem;
            font-size: 0.9rem;
        }

        .breadcrumbs a {
            color: var(--tutorial-color);
            text-decoration: none;
        }

        .breadcrumbs a:hover {
            text-decoration: underline;
        }

        .content {
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 2rem 4rem 2rem;
        }

        .edit-link {
            text-align: right;
            margin-bottom: 1rem;
            font-size: 0.9rem;
        }

        .edit-link a {
            color: var(--secondary-color);
            text-decoration: none;
        }

        .edit-link a:hover {
            text-decoration: underline;
        }

        .content h1 {
            color: var(--tutorial-color);
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 3px solid var(--tutorial-color);
        }

        .content .subtitle {
            color: var(--text-light);
            font-size: 1.2rem;
            margin-bottom: 2rem;
            font-weight: 300;
        }

        .content h2 {
            color: var(--tutorial-color);
            font-size: 1.8rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            padding-bottom: 0.3rem;
            border-bottom: 2px solid var(--border-color);
        }

        .content h3 {
            color: var(--text-dark);
            font-size: 1.3rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .content p {
            margin-bottom: 1rem;
            color: var(--text-dark);
        }

        .content ul, .content ol {
            margin-bottom: 1rem;
            margin-left: 2rem;
        }

        .content li {
            margin-bottom: 0.5rem;
        }

        .content code {
            background: var(--bg-light);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Ubuntu Mono', monospace;
            font-size: 0.9em;
        }

        .content pre {
            background: #0d1117;
            color: #c9d1d9;
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1rem;
        }

        .content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 0.9rem;
        }

        .content a {
            color: var(--secondary-color);
            text-decoration: none;
        }

        .content a:hover {
            text-decoration: underline;
        }

        .note, .warning, .success {
            padding: 1rem;
            border-radius: 6px;
            margin-bottom: 1rem;
            border-left: 4px solid;
        }

        .note {
            background: #e7f2fa;
            border-color: var(--secondary-color);
        }

        .note strong {
            color: var(--secondary-color);
        }

        .warning {
            background: #fff4e6;
            border-color: var(--warning-color);
        }

        .warning strong {
            color: var(--warning-color);
        }

        .success {
            background: #e6f4ea;
            border-color: var(--success-color);
        }

        .success strong {
            color: var(--success-color);
        }

        .step-box {
            background: var(--bg-light);
            border-left: 4px solid var(--tutorial-color);
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            border-radius: 6px;
        }

        .step-number {
            display: inline-block;
            background: var(--tutorial-color);
            color: white;
            width: 32px;
            height: 32px;
            border-radius: 50%;
            text-align: center;
            line-height: 32px;
            font-weight: bold;
            margin-right: 0.5rem;
        }

        .architecture-diagram {
            background: var(--bg-light);
            padding: 1.5rem;
            border-radius: 6px;
            margin: 1.5rem 0;
            font-family: 'Ubuntu Mono', monospace;
            font-size: 0.85rem;
            overflow-x: auto;
        }

        .architecture-diagram pre {
            background: none;
            padding: 0;
            margin: 0;
            color: var(--text-dark);
        }

        footer {
            background: var(--bg-light);
            padding: 2rem;
            text-align: center;
            color: var(--text-light);
            margin-top: 4rem;
        }

        footer a {
            color: var(--primary-color);
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        .footer-links {
            margin-top: 1rem;
        }

        .footer-links a {
            margin: 0 1rem;
        }

        @media (max-width: 768px) {
            .content {
                padding: 0 1rem 2rem 1rem;
            }

            header h1 {
                font-size: 1.5rem;
            }

            .content h1 {
                font-size: 2rem;
            }

            .content h2 {
                font-size: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Concourse CI Machine Charm</h1>
        <p>Documentation</p>
    </header>

    <nav class="breadcrumbs">
        <a href="../index.html">Home</a> &gt; 
        <a href="../index.html#tutorials">Tutorials</a> &gt; 
        <span>Mounting Datasets for ML Tasks</span>
    </nav>

    <main class="content">
        <div class="edit-link">
            <a href="https://github.com/fourdollars/concourse-ci-machine/edit/main/docs/tutorials/dataset-mounting.html">‚úèÔ∏è Edit on GitHub</a>
        </div>

        <h1>Mounting Datasets for ML Tasks</h1>
        <p class="subtitle">Learn how to automatically inject datasets into your Concourse CI GPU tasks using the OCI runtime wrapper</p>

        <div class="note">
            <strong>üí° What you'll learn:</strong>
            <ul>
                <li>How the automatic dataset injection system works</li>
                <li>Setting up LXC disk mounts for datasets</li>
                <li>Accessing datasets in your ML pipelines</li>
                <li>Testing and verifying dataset availability</li>
            </ul>
        </div>

        <div class="note">
            <strong>üìö Prerequisites:</strong>
            <ul>
                <li>Completed the <a href="gpu-workers.html">GPU Workers tutorial</a></li>
                <li>GPU-enabled worker deployed and running</li>
                <li>Dataset files stored on your host machine</li>
            </ul>
        </div>

        <h2>How It Works</h2>

        <p>The charm includes an intelligent OCI runtime wrapper that <strong>automatically discovers and injects all folders under <code>/srv</code></strong> into every task container. This means:</p>

        <ul>
            <li>‚úÖ <strong>Zero configuration in pipelines</strong> - no need to modify your YAML files</li>
            <li>‚úÖ <strong>Automatic discovery</strong> - any folder mounted to <code>/srv/*</code> becomes available</li>
            <li>‚úÖ <strong>Read-only by default</strong> - prevents accidental data corruption</li>
            <li>‚úÖ <strong>Works with GPU and non-GPU workers</strong></li>
        </ul>

        <div class="architecture-diagram">
            <pre>Host Machine                    LXC Container                  Task Container
‚îú‚îÄ‚îÄ /data/datasets/            ‚îú‚îÄ‚îÄ /srv/datasets/             ‚îú‚îÄ‚îÄ /srv/datasets/
‚îÇ   ‚îú‚îÄ‚îÄ training/              ‚îÇ   ‚îú‚îÄ‚îÄ training/              ‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ validation/            ‚îÇ   ‚îú‚îÄ‚îÄ validation/            ‚îÇ   ‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îî‚îÄ‚îÄ test/                  ‚îÇ   ‚îî‚îÄ‚îÄ test/                  ‚îÇ   ‚îî‚îÄ‚îÄ test/
                                ‚îÇ                              ‚îÇ
                                ‚îú‚îÄ‚îÄ OCI Wrapper                ‚îÇ
                                ‚îÇ   (runc-gpu-wrapper)         ‚îÇ
                                ‚îÇ   ‚Ä¢ Discovers /srv/*         ‚îÇ
                                ‚îÇ   ‚Ä¢ Injects bind mounts      ‚îÇ
                                ‚îÇ   ‚Ä¢ Sets read-only          ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&gt; ‚îî‚îÄ‚îÄ Auto-mounted!</pre>
        </div>

        <h2>Step-by-Step Setup</h2>

        <div class="step-box">
            <p><span class="step-number">1</span> <strong>Prepare your dataset directory on the host</strong></p>
            <pre><code class="language-bash"># Create a directory structure for your datasets
mkdir -p /data/ml-datasets/imagenet
mkdir -p /data/ml-datasets/validation

# Copy your dataset files
cp -r /path/to/your/training-data/* /data/ml-datasets/imagenet/

# Verify permissions (datasets should be readable)
chmod -R a+rX /data/ml-datasets/</code></pre>
        </div>

        <div class="note">
            <strong>üí° Tip:</strong> Use descriptive directory names. Whatever you mount to <code>/srv/datasets</code> will be available at that exact path in your tasks.
        </div>

        <div class="step-box">
            <p><span class="step-number">2</span> <strong>Find your GPU worker's LXC container name</strong></p>
            <pre><code class="language-bash"># List all LXC containers
lxc list

# Look for containers with "juju" prefix
# Example output:
# +----------------+---------+
# | NAME           | STATE   |
# +----------------+---------+
# | juju-abc123-4  | RUNNING |  &lt;-- This is your GPU worker
# +----------------+---------+

# Alternative: Get it directly from Juju
juju status gpu-worker --format=json | jq -r '.machines | to_entries[] | .value."container-id"'</code></pre>
        </div>

        <div class="step-box">
            <p><span class="step-number">3</span> <strong>Mount the dataset directory into the LXC container</strong></p>
            <pre><code class="language-bash"># Replace &lt;container-name&gt; with your actual container name
# Mount your dataset to /srv/datasets (read-only)
lxc config device add &lt;container-name&gt; datasets disk \
  source=/data/ml-datasets \
  path=/srv/datasets \
  readonly=true

# Example:
lxc config device add juju-abc123-4 datasets disk \
  source=/data/ml-datasets \
  path=/srv/datasets \
  readonly=true</code></pre>
        </div>

        <div class="warning">
            <strong>‚ö†Ô∏è Important:</strong> The path inside the container MUST be under <code>/srv/</code> for automatic discovery. Use <code>/srv/datasets</code>, <code>/srv/models</code>, <code>/srv/data</code>, etc.
        </div>

        <div class="step-box">
            <p><span class="step-number">4</span> <strong>Verify the mount inside the container</strong></p>
            <pre><code class="language-bash"># Check the LXC device configuration
lxc config device show &lt;container-name&gt;

# Expected output:
# datasets:
#   path: /srv/datasets
#   readonly: "true"
#   source: /data/ml-datasets
#   type: disk

# Verify the files are visible inside the container
lxc exec &lt;container-name&gt; -- ls -lah /srv/datasets/

# You should see your dataset files listed</code></pre>
        </div>

        <div class="success">
            <strong>‚úÖ Setup complete!</strong> The OCI wrapper will now automatically inject <code>/srv/datasets</code> into every task container. No pipeline changes needed!
        </div>

        <h2>Using Datasets in Pipelines</h2>

        <p>Now that your datasets are mounted, they're automatically available in any task that runs on your GPU worker. Here's a complete example:</p>

        <h3>Example 1: PyTorch Training Pipeline</h3>

        <pre><code class="language-yaml">jobs:
  - name: train-model
    plan:
      - task: training
        tags: [cuda]  # Target GPU workers
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: pytorch/pytorch
              tag: latest
          run:
            path: python3
            args:
              - -c
              - |
                import os
                import torch
                
                # Dataset is automatically available!
                print("Available datasets:")
                print(os.listdir('/srv/datasets'))
                
                # Check GPU availability
                print(f"\nGPU Available: {torch.cuda.is_available()}")
                print(f"GPU Device: {torch.cuda.get_device_name(0)}")
                
                # Your training code here
                # data_dir = '/srv/datasets/imagenet'
                # model = train_model(data_dir)</code></pre>

        <h3>Example 2: Verification Task</h3>

        <p>Create a simple task to verify dataset access before running expensive training jobs:</p>

        <pre><code class="language-yaml">jobs:
  - name: verify-datasets
    plan:
      - task: check-datasets
        tags: [cuda]
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: ubuntu
              tag: latest
          run:
            path: bash
            args:
              - -c
              - |
                echo "=== Dataset Verification ==="
                
                # Check mount exists
                if [ -d "/srv/datasets" ]; then
                  echo "‚úÖ /srv/datasets exists"
                else
                  echo "‚ùå /srv/datasets not found"
                  exit 1
                fi
                
                # Check read access
                echo "\nDataset contents:"
                ls -lah /srv/datasets/
                
                # Count files
                file_count=$(find /srv/datasets -type f | wc -l)
                echo "\nTotal files: $file_count"
                
                # Verify read-only (should fail)
                if touch /srv/datasets/write-test 2&gt;&amp;1 | grep -q "Read-only"; then
                  echo "‚úÖ Confirmed read-only mount"
                else
                  echo "‚ö†Ô∏è  Warning: Mount may not be read-only"
                fi
                
                echo "\n‚úÖ Dataset verification passed!"</code></pre>

        <h3>Example 3: Multi-Dataset Pipeline</h3>

        <p>If you have multiple dataset directories mounted, access them by their paths:</p>

        <pre><code class="language-bash"># On host: Mount multiple datasets
lxc config device add juju-abc123-4 training-data disk \
  source=/data/training \
  path=/srv/datasets/training \
  readonly=true

lxc config device add juju-abc123-4 validation-data disk \
  source=/data/validation \
  path=/srv/datasets/validation \
  readonly=true</code></pre>

        <pre><code class="language-yaml">jobs:
  - name: train-and-validate
    plan:
      - task: ml-workflow
        tags: [cuda]
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: pytorch/pytorch
              tag: latest
          run:
            path: python3
            args:
              - -c
              - |
                # All datasets automatically available
                train_data = '/srv/datasets/training'
                val_data = '/srv/datasets/validation'
                
                print(f"Training samples: {len(os.listdir(train_data))}")
                print(f"Validation samples: {len(os.listdir(val_data))}")
                
                # Your ML workflow here</code></pre>

        <h2>Advanced: Writable Output Directories</h2>

        <p>By default, all mounts are read-only for data safety. If you need to write model outputs, checkpoints, or results, use the <code>_writable</code> or <code>_rw</code> suffix:</p>

        <pre><code class="language-bash"># Mount a writable directory for model outputs
lxc config device add juju-abc123-4 models disk \
  source=/data/model-outputs \
  path=/srv/models_writable \
  readonly=false

# The _writable suffix tells the wrapper to mount it read-write</code></pre>

        <div class="warning">
            <strong>‚ö†Ô∏è Security Note:</strong> Only use writable mounts when necessary. Read-only mounts prevent tasks from accidentally corrupting your training data.
        </div>

        <pre><code class="language-yaml">jobs:
  - name: train-and-save
    plan:
      - task: training
        tags: [cuda]
        config:
          platform: linux
          image_resource:
            type: registry-image
            source:
              repository: pytorch/pytorch
              tag: latest
          run:
            path: python3
            args:
              - -c
              - |
                import torch
                
                # Read from read-only dataset
                train_data = '/srv/datasets/training'
                
                # Write to writable directory
                output_dir = '/srv/models_writable'
                
                # Train and save
                model = train_model(train_data)
                torch.save(model.state_dict(), f'{output_dir}/model.pth')
                print(f"Model saved to {output_dir}/model.pth")</code></pre>

        <h2>Troubleshooting</h2>

        <h3>Dataset not visible in tasks</h3>

        <p><strong>Check 1: Verify LXC mount</strong></p>
        <pre><code class="language-bash">lxc config device show &lt;container-name&gt;
lxc exec &lt;container-name&gt; -- ls -la /srv/datasets/</code></pre>

        <p><strong>Check 2: Ensure path is under /srv</strong></p>
        <p>The OCI wrapper only discovers folders under <code>/srv/</code>. Paths like <code>/mnt/datasets</code> or <code>/data</code> will NOT work.</p>

        <p><strong>Check 3: Verify OCI wrapper is installed</strong></p>
        <pre><code class="language-bash">juju ssh gpu-worker/0 -- cat /usr/local/bin/runc-gpu-wrapper | grep -A5 "discover.*srv"</code></pre>

        <h3>Permission denied errors</h3>

        <pre><code class="language-bash"># Make dataset directory readable by all users
chmod -R a+rX /data/ml-datasets/

# Check ownership (files should be readable)
ls -la /data/ml-datasets/</code></pre>

        <h3>Mount not updating after changes</h3>

        <p>If you change dataset contents but tasks see old data:</p>

        <pre><code class="language-bash"># Restart the worker to refresh mounts
juju ssh gpu-worker/0 -- sudo systemctl restart concourse-worker

# Or restart the entire container
lxc restart &lt;container-name&gt;</code></pre>

        <h2>What You've Accomplished</h2>

        <div class="success">
            <strong>üéâ Congratulations!</strong> You've learned:
            <ul>
                <li>‚úÖ How the automatic dataset injection system works</li>
                <li>‚úÖ Setting up LXC disk mounts for your datasets</li>
                <li>‚úÖ Accessing datasets in your ML pipelines without configuration</li>
                <li>‚úÖ Using read-only and writable mounts appropriately</li>
                <li>‚úÖ Verifying and troubleshooting dataset availability</li>
            </ul>
        </div>

        <h2>Next Steps</h2>

        <p>Now that you can mount datasets, explore more advanced topics:</p>

        <ul>
            <li><strong><a href="general-mounting.html">General Folder Mounting</a></strong> - Mount any type of folder (caches, configs, artifacts) to any worker</li>
            <li><strong><a href="../howto/monitor-prometheus.html">Set up monitoring</a></strong> - Track GPU utilization and training metrics</li>
            <li><strong><a href="../reference/configuration.html">Configuration Reference</a></strong> - Learn about all available options</li>
        </ul>

        <div class="note">
            <strong>üí° Best Practice:</strong> Always start with a verification task to ensure datasets are accessible before running expensive training jobs. Save time and GPU resources by catching mount issues early!
        </div>
    </main>

    <footer>
        <p>&copy; 2026 Concourse CI Machine Charm | Maintained by <a href="https://github.com/fourdollars">Shih-Yuan Lee (FourDollars)</a></p>
        <div class="footer-links">
            <a href="https://github.com/fourdollars/concourse-ci-machine">GitHub</a>
            <a href="https://charmhub.io/concourse-ci-machine">Charmhub</a>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</body>
</html>
