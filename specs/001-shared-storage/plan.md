# Implementation Plan: Shared Storage for Concourse CI Units

**Branch**: `001-shared-storage` | **Date**: 2026-01-12 | **Spec**: [spec.md](./spec.md)  
**Input**: Feature specification from `/specs/001-shared-storage/spec.md`

**Note**: This implementation plan is generated by the `/speckit.plan` command.

## Summary

Implement shared storage architecture for Concourse CI charm to eliminate redundant binary downloads across multiple units. The primary requirement is to configure Juju storage so that all Concourse units mount the same volume for binaries, reducing disk overhead from N×binary size to ~1.15× while maintaining per-unit worker data isolation.

**Technical Approach** (from Phase 0 research):
- Modify metadata.yaml storage configuration to support shared/pooled storage mode
- Implement file-based locking using Python's fcntl for concurrent write protection
- Update concourse_installer.py to detect existing binaries before downloading
- Add storage coordination logic to charm lifecycle events (install, config-changed, upgrade)
- Maintain backward compatibility with existing single-unit non-shared deployments

## Technical Context

**Language/Version**: Python 3.11+ (Ubuntu 24.04 base)  
**Primary Dependencies**: ops framework ≥2.0.0, fcntl (stdlib), pathlib (stdlib)  
**Storage**: Juju storage (filesystem-based, shared across units via pool configuration)  
**Testing**: E2E deployment tests with Juju 3.6+ and LXD 5.21+, GitHub Actions CI  
**Target Platform**: Ubuntu 24.04 LTS (Juju-managed machines/containers)  
**Project Type**: Single project (Juju charm with modular lib/ structure)  
**Performance Goals**: Binary download once per cluster, <3min unit addition with existing binaries  
**Constraints**: Filesystem-level locking only (no distributed coordination), POSIX-compliant filesystem required  
**Scale/Scope**: Support 1-10 units initially, extensible to larger deployments

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

Verify compliance with constitution principles (v1.0.1):

- [x] **Code Quality Standards**: Type hints for all new functions, specific exception handling, no hardcoded paths
- [x] **Testing Discipline**: E2E tests for shared storage deployment modes, file locking edge cases
- [x] **User Experience Consistency**: Clear status messages during storage coordination, error guidance for lock failures
- [x] **Performance Requirements**: <3min unit addition (vs 5min baseline), <2min upgrade with shared binaries

**Complexity Justifications**:
- None required - implementation follows existing charm patterns with additional storage coordination logic

## Project Structure

### Documentation (this feature)

```text
specs/001-shared-storage/
├── spec.md              # Feature specification (already created)
├── plan.md              # This file (implementation plan)
├── research.md          # Phase 0: Juju storage patterns, locking mechanisms
├── data-model.md        # Phase 1: Storage entities and state transitions
├── quickstart.md        # Phase 1: Deployment guide for shared storage
├── contracts/           # Phase 1: Storage coordinator interface
│   ├── storage_coordinator.py  # Lock file contract specification
│   └── upgrade_protocol.py     # Upgrade coordination interface
└── checklists/
    └── requirements.md  # Quality validation checklist (already created)
```

### Source Code (repository root)

```text
concourse-ci-machine/
├── src/
│   ├── charm.py                      # Main charm logic (modify: storage hooks)
│   └── folder_mount_manager.py       # (existing, no changes needed)
│
├── lib/
│   ├── concourse_common.py           # (modify: add storage coordination utils)
│   ├── concourse_installer.py        # (modify: detect existing binaries)
│   ├── concourse_web.py              # (modify: shared storage awareness)
│   ├── concourse_worker.py           # (modify: shared storage awareness)
│   ├── concourse_helper.py           # (existing, no changes needed)
│   └── storage_coordinator.py        # (NEW: file locking and coordination)
│
├── metadata.yaml                     # (modify: storage configuration)
├── config.yaml                       # (existing, no new config options)
│
├── tests/
│   ├── unit/
│   │   ├── test_storage_coordinator.py  # NEW: lock acquisition/release tests
│   │   └── test_installer_shared.py     # NEW: binary detection logic tests
│   │
│   └── integration/  # Handled by existing .github/workflows/ci.yml
│       └── (E2E tests will be added to CI workflow)
│
└── .github/workflows/
    └── ci.yml  # (modify: add shared storage test scenarios)
```

**Structure Decision**: Single project structure (Juju charm). The charm already uses a modular library approach in `lib/` with separate modules for web, worker, installer, and common utilities. This feature adds a new `storage_coordinator.py` module and modifies existing modules to add shared storage awareness. No changes to the overall project layout are required.

## Testing Strategy

### E2E Deployment Tests (Primary Validation)

**Test Infrastructure**:
- GitHub Actions with LXD 5.21+ containers
- Juju 3.6+ bootstrap on localhost provider
- Shared NFS/Ceph storage pool configuration
- Multi-unit deployment scenarios (3+ units)

**Test Scenarios** (extend existing `.github/workflows/ci.yml`):

### Core Shared Storage Tests (New Jobs)

1. **test-shared-storage-auto-mode**: Deploy 3 units in `mode=auto` with shared storage
   - Deploy with `--attach-storage` for units 1 and 2
   - Verify leader (unit 0) downloads binaries once
   - Verify workers (units 1, 2) detect and reuse existing binaries
   - Measure disk usage across units (target: <1.2x binary size total)
   - Verify binary download count (expect: exactly 1 download)
   - Test upgrade coordination: leader initiates, workers auto-upgrade
   - Measure upgrade duration (target: <2 minutes)

2. **test-shared-storage-all-mode**: Deploy single unit in `mode=all` with storage, then scale
   - Deploy unit 0 in `mode=all` (both web and worker on same unit)
   - Add unit 1 with `--attach-storage` (should detect existing binaries)
   - Verify both units share same storage volume
   - Verify only 1 download despite 2 complete Concourse installations
   - Test that both web servers can start (on different ports if configured)

3. **test-shared-storage-web-worker-mode**: Separate web and worker apps with shared storage
   - Deploy `web` app in `mode=web` with storage
   - Deploy `worker` app in `mode=worker` with `--attach-storage` to web's storage
   - Add second worker app instance with same attached storage
   - Verify web downloads binaries once
   - Verify all workers reuse web's binaries
   - Verify TSA relation still works with shared storage
   - Test upgrade: web upgrades → all workers auto-detect new version

4. **test-new-unit-addition**: Deploy cluster, then add unit with existing binaries
   - Deploy 2 units in `mode=auto` with shared storage at v7.14.2
   - Wait for settling (binaries downloaded)
   - Add 3rd unit with `--attach-storage`
   - Verify new unit detects existing binaries (no download)
   - Measure unit addition time (target: <3 minutes vs 5min baseline)
   - Verify new worker registers with TSA correctly

5. **test-storage-contention**: Concurrent operations with shared storage
   - Deploy 3 units with shared storage
   - Simultaneously trigger config-changed on multiple units
   - Verify no file corruption or lock failures
   - Verify proper lock acquisition/release in logs
   - Test concurrent unit additions (add 2 units simultaneously)
   - Verify only one downloads, other waits

6. **test-storage-failure-recovery**: Storage unavailability scenarios
   - Deploy with shared storage
   - Simulate storage mount failure (unmount from one unit)
   - Verify charm enters blocked state with clear error message
   - Remount storage, trigger update-status
   - Verify recovery and return to active state

### Integration with Existing Mode Tests

The existing CI jobs should be enhanced to optionally test with shared storage:

- **test-all-mode** (existing): Keep as-is (non-shared storage baseline)
- **test-auto-mode** (existing): Keep as-is (non-shared storage baseline)
- **test-web-worker-mode** (existing): Keep as-is (non-shared storage baseline)

This maintains backward compatibility testing while new jobs validate shared storage.

### LXC/Juju Testing Requirements

**LXC Configuration**:
- Use canonical/setup-lxd@v1 action for LXD setup
- Leverage LXD's built-in rootfs storage for shared testing (no external NFS needed)
- Use `--attach-storage` to share storage volumes across Juju units in same LXC containers
- Configure LXD storage pools with appropriate backing store (dir, zfs, btrfs)

**Shared Storage Testing Approach**:
Instead of requiring external NFS setup, use LXD's native storage capabilities with `--attach-storage`:

```bash
# Standard approach for all modes with shared storage testing
juju bootstrap localhost test-controller --config test-mode=true

# MODE=AUTO testing (leader=web, non-leaders=workers)
juju deploy ./charm.charm concourse-ci \
  --storage concourse-data=10G \
  --config mode=auto --config version=7.14.2

# Wait for unit 0 (leader) to settle and download binaries
juju-wait -t 600

# Get storage ID from first unit
STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("concourse-ci/0")) | .key')

# Add additional units sharing the same storage
juju add-unit concourse-ci --attach-storage $STORAGE_ID
juju add-unit concourse-ci --attach-storage $STORAGE_ID

# MODE=ALL testing (each unit runs both web and worker)
juju deploy ./charm.charm concourse-ci \
  --storage concourse-data=10G \
  --config mode=all --config version=7.14.2

# Get storage ID
STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("concourse-ci/0")) | .key')

# Add second unit sharing storage (scales to 2 complete Concourse instances)
juju add-unit concourse-ci --attach-storage $STORAGE_ID

# MODE=WEB+WORKER testing (separate apps)
juju deploy ./charm.charm web \
  --storage concourse-data=10G \
  --config mode=web --config version=7.14.2

# Get web's storage ID
WEB_STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("web/0")) | .key')

# Deploy workers sharing web's storage
juju deploy ./charm.charm worker \
  --storage concourse-data=$WEB_STORAGE_ID \
  --config mode=worker --config version=7.14.2

# Add more workers sharing the same storage
juju add-unit worker --attach-storage $WEB_STORAGE_ID

# Relate for TSA communication
juju relate worker:worker-tsa web:web-tsa
```

**Juju Testing Commands**:
```bash
# Verify storage attachment across units
juju storage --format=json | jq '.storage'

# Check that all units see the same filesystem
for unit in 0 1 2; do
  echo "=== Unit $unit ==="
  juju exec --unit concourse-ci/$unit -- df -h /var/lib/concourse
  juju exec --unit concourse-ci/$unit -- stat -f /var/lib/concourse
  juju exec --unit concourse-ci/$unit -- ls -lah /var/lib/concourse/bin
done

# Verify filesystem ID matches across units (shared mount)
for unit in 0 1 2; do
  juju exec --unit concourse-ci/$unit -- stat -f -c '%i' /var/lib/concourse
done | sort -u | wc -l  # Should output: 1 (same filesystem)

# Check binary presence and version markers
for unit in 0 1 2; do
  juju exec --unit concourse-ci/$unit -- cat /var/lib/concourse/.installed_version
done

# Verify download logs show single download
juju debug-log --replay --no-tail | grep "Downloading Concourse" | wc -l  # Should be: 1
```

**Storage Validation Checks**:
- Filesystem ID consistency across units
- Binary file count (should be same across all units)
- Lock file presence during downloads
- Version marker file accuracy
- Worker directory isolation (each unit has own subdir)

### GitHub Actions Workflow Extensions

Add multiple new jobs to `.github/workflows/ci.yml` to test all deployment modes with shared storage:

```yaml
# Job 1: Test mode=auto with shared storage
test-shared-storage-auto:
  name: Test shared storage (mode=auto)
  runs-on: ubuntu-latest
  timeout-minutes: 35
  needs: build-charm
  steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Download charm artifact
      uses: actions/download-artifact@v4
      with:
        name: concourse-ci-machine-charm
    
    - name: Setup LXD
      uses: canonical/setup-lxd@v1
      with:
        channel: 5.21/stable
    
    - name: Configure LXD IPv6
      run: |
        lxc network set lxdbr0 ipv6.address none
        lxc network set lxdbr0 ipv6.dhcp false
        lxc network set lxdbr0 ipv6.nat false
    
    - name: Install Juju
      run: |
        sudo snap install juju --channel 3.6/stable
        sudo snap install juju-wait --classic
    
    - name: Bootstrap Juju
      run: juju bootstrap localhost test-controller --config test-mode=true
    
    - name: Add Juju Model
      run: juju add-model shared-auto
    
    - name: Deploy first unit with storage (becomes leader/web)
      run: |
        juju deploy ./concourse-ci-machine_amd64.charm concourse-ci \
          --storage concourse-data=10G \
          --config mode=auto --config version=7.14.2
        juju deploy postgresql --channel 16/stable
        juju relate concourse-ci:postgresql postgresql:database
    
    - name: Wait for first unit to settle and download binaries
      run: juju-wait -m shared-auto -t 600
    
    - name: Verify initial deployment
      run: |
        echo "=== Unit 0 should be leader running web server ==="
        juju status
        juju exec --unit concourse-ci/0 -- systemctl status concourse-server
    
    - name: Get storage ID and add units with shared storage
      run: |
        echo "=== Getting storage ID from unit 0 ==="
        STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("concourse-ci/0")) | .key')
        echo "Storage ID: $STORAGE_ID"
        
        if [ -z "$STORAGE_ID" ]; then
          echo "ERROR: Could not find storage ID"
          exit 1
        fi
        
        echo "=== Adding worker units 1 and 2 with attached storage ==="
        juju add-unit concourse-ci --attach-storage $STORAGE_ID
        juju add-unit concourse-ci --attach-storage $STORAGE_ID
    
    - name: Wait for all units to settle
      run: juju-wait -m shared-auto -t 900
    
    - name: Verify shared storage configuration
      run: |
        echo "=== Storage attachment info ==="
        juju storage
        
        echo "=== Verifying all units see same filesystem ID ==="
        for unit in 0 1 2; do
          echo "Unit $unit filesystem ID:"
          juju exec --unit concourse-ci/$unit -- stat -f -c '%i' /var/lib/concourse
        done | sort -u | wc -l  # Should output: 1
        
        echo "=== Checking binaries exist on all units ==="
        for unit in 0 1 2; do
          echo "Unit $unit:"
          juju exec --unit concourse-ci/$unit -- ls -lh /var/lib/concourse/bin/concourse
        done
        
        echo "=== Verifying version markers match ==="
        for unit in 0 1 2; do
          juju exec --unit concourse-ci/$unit -- cat /var/lib/concourse/.installed_version
        done | sort -u  # Should show single version
        
        echo "=== Verify single download (check logs) ==="
        DOWNLOADS=$(juju debug-log --replay --no-tail | grep -c "Downloading Concourse binaries" || echo 0)
        echo "Total Concourse downloads: $DOWNLOADS"
        if [ "$DOWNLOADS" -gt 1 ]; then
          echo "⚠️  WARNING: Expected 1 download, found $DOWNLOADS"
        fi
    
    - name: Verify workers are running
      run: |
        echo "=== Checking worker services ==="
        juju exec --unit concourse-ci/1 -- systemctl status concourse-worker
        juju exec --unit concourse-ci/2 -- systemctl status concourse-worker
    
    - name: Test upgrade with shared storage
      run: |
        echo "=== Upgrading to 7.14.3 ==="
        juju config concourse-ci version=7.14.3
        juju run concourse-ci/leader upgrade version=7.14.3
        
        sleep 30
        juju-wait -m shared-auto -t 600
        
        echo "=== Verifying all units upgraded ==="
        juju status concourse-ci
        
        UNIT_COUNT=$(juju status concourse-ci --format=json | jq '.applications."concourse-ci".units | length')
        VERSION_COUNT=$(juju status concourse-ci --format=json | jq -r '.applications."concourse-ci".units[].["workload-status"].message' | grep -c "7.14.3" || echo 0)
        
        if [ "$VERSION_COUNT" -ne "$UNIT_COUNT" ]; then
          echo "❌ Upgrade failed: $VERSION_COUNT/$UNIT_COUNT units upgraded"
          exit 1
        fi
        echo "✅ All units upgraded successfully"
    
    - name: Cleanup
      if: always()
      run: |
        yes shared-auto | juju destroy-model shared-auto --destroy-storage --force --no-wait || true
        rm concourse-ci-machine_amd64.charm || true

# Job 2: Test mode=all with shared storage
test-shared-storage-all:
  name: Test shared storage (mode=all)
  runs-on: ubuntu-latest
  timeout-minutes: 35
  needs: build-charm
  steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Download charm artifact
      uses: actions/download-artifact@v4
      with:
        name: concourse-ci-machine-charm
    
    - name: Setup LXD
      uses: canonical/setup-lxd@v1
      with:
        channel: 5.21/stable
    
    - name: Configure LXD IPv6
      run: |
        lxc network set lxdbr0 ipv6.address none
        lxc network set lxdbr0 ipv6.dhcp false
        lxc network set lxdbr0 ipv6.nat false
    
    - name: Install Juju
      run: |
        sudo snap install juju --channel 3.6/stable
        sudo snap install juju-wait --classic
    
    - name: Bootstrap Juju
      run: juju bootstrap localhost test-controller --config test-mode=true
    
    - name: Add Juju Model
      run: juju add-model shared-all
    
    - name: Deploy first unit (mode=all with both web and worker)
      run: |
        juju deploy ./concourse-ci-machine_amd64.charm concourse-ci \
          --storage concourse-data=10G \
          --config mode=all --config version=7.14.2
        juju deploy postgresql --channel 16/stable
        juju relate concourse-ci:postgresql postgresql:database
    
    - name: Wait for first unit to settle
      run: juju-wait -m shared-all -t 600
    
    - name: Verify first unit runs both server and worker
      run: |
        juju exec --unit concourse-ci/0 -- systemctl status concourse-server
        juju exec --unit concourse-ci/0 -- systemctl status concourse-worker
    
    - name: Add second unit with shared storage
      run: |
        STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("concourse-ci/0")) | .key')
        echo "Attaching unit 1 to storage: $STORAGE_ID"
        juju add-unit concourse-ci --attach-storage $STORAGE_ID
    
    - name: Wait for second unit
      run: juju-wait -m shared-all -t 600
    
    - name: Verify shared storage
      run: |
        echo "=== Both units should see same binaries ==="
        for unit in 0 1; do
          echo "Unit $unit:"
          juju exec --unit concourse-ci/$unit -- ls -lh /var/lib/concourse/bin/concourse
          juju exec --unit concourse-ci/$unit -- cat /var/lib/concourse/.installed_version
        done
        
        echo "=== Verify second unit didn't re-download ==="
        DOWNLOADS=$(juju debug-log --replay --no-tail | grep -c "Downloading Concourse binaries" || echo 0)
        echo "Total downloads: $DOWNLOADS (expected: 1)"
        if [ "$DOWNLOADS" -gt 1 ]; then
          echo "⚠️  Unit 1 may have re-downloaded (expected reuse)"
        fi
    
    - name: Cleanup
      if: always()
      run: |
        yes shared-all | juju destroy-model shared-all --destroy-storage --force --no-wait || true
        rm concourse-ci-machine_amd64.charm || true

# Job 3: Test mode=web+worker with shared storage
test-shared-storage-web-worker:
  name: Test shared storage (mode=web+worker)
  runs-on: ubuntu-latest
  timeout-minutes: 35
  needs: build-charm
  steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Download charm artifact
      uses: actions/download-artifact@v4
      with:
        name: concourse-ci-machine-charm
    
    - name: Setup LXD
      uses: canonical/setup-lxd@v1
      with:
        channel: 5.21/stable
    
    - name: Configure LXD IPv6
      run: |
        lxc network set lxdbr0 ipv6.address none
        lxc network set lxdbr0 ipv6.dhcp false
        lxc network set lxdbr0 ipv6.nat false
    
    - name: Install Juju
      run: |
        sudo snap install juju --channel 3.6/stable
        sudo snap install juju-wait --classic
    
    - name: Bootstrap Juju
      run: juju bootstrap localhost test-controller --config test-mode=true
    
    - name: Add Juju Model
      run: juju add-model shared-web-worker
    
    - name: Deploy web server with storage
      run: |
        juju deploy ./concourse-ci-machine_amd64.charm web \
          --storage concourse-data=10G \
          --config mode=web --config version=7.14.2
        juju deploy postgresql --channel 16/stable
        juju relate web:postgresql postgresql:database
    
    - name: Wait for web to settle and download binaries
      run: juju-wait -m shared-web-worker -t 600
    
    - name: Get web's storage ID
      run: |
        WEB_STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("web/0")) | .key')
        echo "WEB_STORAGE_ID=$WEB_STORAGE_ID" >> $GITHUB_ENV
        echo "Web storage ID: $WEB_STORAGE_ID"
    
    - name: Deploy workers with shared storage
      run: |
        # Deploy first worker sharing web's storage
        juju deploy ./concourse-ci-machine_amd64.charm worker \
          --storage concourse-data=${{ env.WEB_STORAGE_ID }} \
          --config mode=worker --config version=7.14.2
        
        # Add second worker also sharing storage
        juju add-unit worker --attach-storage ${{ env.WEB_STORAGE_ID }}
    
    - name: Relate workers to web via TSA
      run: juju relate worker:worker-tsa web:web-tsa
    
    - name: Wait for workers to settle
      run: juju-wait -m shared-web-worker -t 900
    
    - name: Verify shared storage across apps
      run: |
        echo "=== Verifying web has binaries ==="
        juju exec --unit web/0 -- ls -lh /var/lib/concourse/bin/concourse
        juju exec --unit web/0 -- cat /var/lib/concourse/.installed_version
        
        echo "=== Verifying workers see same binaries ==="
        for unit in 0 1; do
          echo "Worker unit $unit:"
          juju exec --unit worker/$unit -- ls -lh /var/lib/concourse/bin/concourse
          juju exec --unit worker/$unit -- cat /var/lib/concourse/.installed_version
        done
        
        echo "=== Check download count (should be 1) ==="
        DOWNLOADS=$(juju debug-log --replay --no-tail | grep -c "Downloading Concourse binaries" || echo 0)
        echo "Total downloads: $DOWNLOADS"
        if [ "$DOWNLOADS" -gt 1 ]; then
          echo "⚠️  Multiple downloads detected (expected: 1)"
        fi
    
    - name: Test upgrade coordination
      run: |
        echo "=== Upgrading web to 7.14.3 ==="
        juju config web version=7.14.3
        juju config worker version=7.14.3
        juju run web/leader upgrade version=7.14.3
        
        sleep 30
        juju-wait -m shared-web-worker -t 600
        
        echo "=== Verify all apps upgraded ==="
        juju status
        
        WEB_VERSION=$(juju status web --format=json | jq -r '.applications.web.units["web/0"]["workload-status"].message' | grep -o "7.14.3" || echo "")
        WORKER_COUNT=$(juju status worker --format=json | jq '.applications.worker.units | length')
        WORKER_UPGRADED=$(juju status worker --format=json | jq -r '.applications.worker.units[].["workload-status"].message' | grep -c "7.14.3" || echo 0)
        
        if [ -z "$WEB_VERSION" ]; then
          echo "❌ Web not upgraded"
          exit 1
        fi
        
        if [ "$WORKER_UPGRADED" -ne "$WORKER_COUNT" ]; then
          echo "❌ Workers not all upgraded: $WORKER_UPGRADED/$WORKER_COUNT"
          exit 1
        fi
        
        echo "✅ All apps upgraded successfully"
    
    - name: Cleanup
      if: always()
      run: |
        yes shared-web-worker | juju destroy-model shared-web-worker --destroy-storage --force --no-wait || true
        rm concourse-ci-machine_amd64.charm || true
```

These jobs should be added to `.github/workflows/ci.yml` alongside the existing mode tests. The `publish-charm` job dependency list should be updated to include these new shared storage tests:

```yaml
publish-charm:
  name: Publish Charm to Edge
  needs: [
    test-all-mode, 
    test-auto-mode, 
    test-web-worker-mode,
    test-shared-storage-auto,      # NEW
    test-shared-storage-all,        # NEW
    test-shared-storage-web-worker  # NEW
  ]
  if: github.ref == 'refs/heads/main'
  # ... rest of job
```
  name: Test shared storage (mode=auto with attach-storage)
  runs-on: ubuntu-latest
  timeout-minutes: 30
  needs: build-charm
  steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Download charm artifact
      uses: actions/download-artifact@v4
      with:
        name: concourse-ci-machine-charm
    
    - name: Setup LXD
      uses: canonical/setup-lxd@v1
      with:
        channel: 5.21/stable
    
    - name: Configure LXD IPv6
      run: |
        lxc network set lxdbr0 ipv6.address none
        lxc network set lxdbr0 ipv6.dhcp false
        lxc network set lxdbr0 ipv6.nat false
    
    - name: Install Juju
      run: |
        sudo snap install juju --channel 3.6/stable
        sudo snap install juju-wait --classic
    
    - name: Bootstrap Juju
      run: juju bootstrap localhost test-controller --config test-mode=true
    
    - name: Add Juju Model
      run: juju add-model shared-storage-test
    
    - name: Deploy first unit with storage
      run: |
        juju deploy ./concourse-ci-machine_amd64.charm concourse-ci \
          --storage concourse-data=10G \
          --config mode=auto --config version=7.14.2
        juju deploy postgresql --channel 16/stable
    
    - name: Wait for first unit to settle
      run: |
        juju-wait -m shared-storage-test -t 600 || true
        sleep 30
    
    - name: Get storage ID and add units with shared storage
      run: |
        echo "=== Getting storage ID from first unit ==="
        STORAGE_ID=$(juju storage --format=json | jq -r '.storage | to_entries[] | select(.value.attachments.units[] | contains("concourse-ci/0")) | .key')
        echo "Storage ID: $STORAGE_ID"
        
        if [ -z "$STORAGE_ID" ]; then
          echo "ERROR: Could not find storage ID for concourse-ci/0"
          juju storage
          exit 1
        fi
        
        echo "=== Adding units 1 and 2 with attached storage ==="
        juju add-unit concourse-ci --attach-storage $STORAGE_ID
        juju add-unit concourse-ci --attach-storage $STORAGE_ID
    
    - name: Remove charm file
      run: rm concourse-ci-machine_amd64.charm
    
    - name: Relate charms
      run: juju relate concourse-ci:postgresql postgresql:database
    
    - name: Wait for model to settle
      run: juju-wait -m shared-storage-test -t 900
    
    - name: Verify shared storage configuration
      run: |
        echo "=== Checking storage attachment ==="
        juju storage --format=json | jq '.storage'
        
        echo "=== Verifying all units see same filesystem ==="
        for unit in 0 1 2; do
          echo "Unit $unit filesystem ID:"
          juju exec --unit concourse-ci/$unit -- stat -f -c '%i' /var/lib/concourse || echo "Unit $unit not ready"
        done
        
        echo "=== Verifying binaries exist on all units ==="
        for unit in 0 1 2; do
          echo "Unit $unit binaries:"
          juju exec --unit concourse-ci/$unit -- ls -lah /var/lib/concourse/bin/ || echo "Unit $unit not ready"
        done
        
        echo "=== Checking version markers match ==="
        for unit in 0 1 2; do
          echo "Unit $unit version:"
          juju exec --unit concourse-ci/$unit -- cat /var/lib/concourse/.installed_version || echo "Unit $unit not ready"
        done
        
        echo "=== Verify single download occurred ==="
        DOWNLOAD_COUNT=$(juju debug-log --replay --no-tail | grep -c "Downloading Concourse binaries" || echo 0)
        echo "Total downloads: $DOWNLOAD_COUNT"
        if [ "$DOWNLOAD_COUNT" -gt 1 ]; then
          echo "WARNING: Expected 1 download, found $DOWNLOAD_COUNT"
          echo "This may indicate shared storage is not working correctly"
        fi
    
    - name: Measure disk usage
      run: |
        echo "=== Disk usage per unit (should be same across all) ==="
        for unit in 0 1 2; do
          echo "Unit $unit:"
          juju exec --unit concourse-ci/$unit -- du -sh /var/lib/concourse/bin || echo "Unit $unit not ready"
        done
        
        echo "=== Binary file inodes (should match if truly shared) ==="
        for unit in 0 1 2; do
          echo "Unit $unit concourse binary inode:"
          juju exec --unit concourse-ci/$unit -- stat -c '%i' /var/lib/concourse/bin/concourse || echo "Unit $unit not ready"
        done
        
        echo "=== Verify shared worker directories are isolated ==="
        for unit in 0 1 2; do
          echo "Unit $unit worker directory:"
          juju exec --unit concourse-ci/$unit -- ls -lah /var/lib/concourse/worker/ || echo "Unit $unit not ready"
        done
    
    - name: Test upgrade with shared storage
      run: |
        echo "=== Current version on all units ==="
        juju status concourse-ci --format=json | jq -r '.applications."concourse-ci".units | to_entries[] | "\(.key): \(.value."workload-status".message)"'
        
        echo "=== Upgrading to 7.14.3 ==="
        juju config concourse-ci version=7.14.3
        juju run concourse-ci/leader upgrade version=7.14.3
        
        echo "=== Waiting for upgrade to complete ==="
        sleep 30
        juju-wait -m shared-storage-test -t 600
        
        echo "=== Verifying all units upgraded ==="
        juju status concourse-ci --format=json | jq -r '.applications."concourse-ci".units | to_entries[] | "\(.key): \(.value."workload-status".message)"'
        
        echo "=== Verifying single download during upgrade ==="
        UPGRADE_DOWNLOADS=$(juju debug-log --replay --no-tail --since 2m | grep -c "Downloading Concourse.*7.14.3" || echo 0)
        echo "Upgrade downloads for v7.14.3: $UPGRADE_DOWNLOADS"
        if [ "$UPGRADE_DOWNLOADS" -gt 1 ]; then
          echo "WARNING: Expected 1 download during upgrade, found $UPGRADE_DOWNLOADS"
        fi
    
    - name: Cleanup
      if: always()
      run: |
        yes shared-storage-test | juju destroy-model shared-storage-test --destroy-storage --force --no-wait || true
```

### Performance Benchmarks

**Measured Targets** (from constitution + feature goals):

#### Baseline (Non-Shared Storage)
- mode=all deployment: ~2:48 minutes (existing CI)
- mode=auto (2 units) deployment: ~3:32 minutes (existing CI)
- mode=web+worker deployment: ~4:06 minutes (existing CI)
- Upgrade (7.14.2 → 7.14.3): ~1:20 minutes (existing CI)

#### Shared Storage Targets
- **mode=auto (3 units)** with shared storage: <5 minutes (vs ~6-7min without)
  - Unit 0 (leader): <3 minutes to active (downloads binaries)
  - Units 1-2 (workers): <2 minutes to active (reuse binaries)
  - Binary download count: Exactly 1

- **mode=all (2 units)** with shared storage: <6 minutes total
  - Unit 0: <3 minutes (downloads binaries)
  - Unit 1: <3 minutes (reuses binaries, no download)
  - Disk savings: ~50% (1x binaries vs 2x)

- **mode=web+worker (1+2)** with shared storage: <7 minutes total
  - Web deployment: <3 minutes (downloads binaries)
  - Worker 1 deployment + relation: <2 minutes (reuses binaries)
  - Worker 2 scale-up: <2 minutes (reuses binaries)
  - Binary download count: Exactly 1

- **Unit addition** (to existing cluster): <3 minutes (vs 5min baseline = 40% improvement)
  - No binary download (detects existing)
  - Worker directory creation only
  - Service start with existing binaries

- **Upgrade with shared storage**: <2 minutes (vs 3min baseline = 33% improvement)
  - Single download for all units
  - Coordinated service stops/starts
  - All units show new version

- **Lock acquisition**: <1 second (non-blocking check)
- **Binary verification**: <5 seconds per unit
- **Stale lock detection**: 10-minute threshold

**CI Test Duration Target**:
- Existing tests: ~17 minutes (build + 3 mode tests)
- New shared storage tests: +3×25min = +75 minutes (if sequential)
- **Optimization**: Run new tests in parallel → +25 minutes maximum
- **Total CI budget**: <45 minutes (within 1-hour GitHub Actions limit)

## Complexity Tracking

> **Note**: No constitution violations detected. This section documents design decisions for transparency.

| Design Decision | Rationale | Alternatives Considered |
|-----------------|-----------|-------------------------|
| Filesystem-based locking (fcntl) | Simple, POSIX-standard, no external dependencies | Distributed locks (Redis/etcd) - rejected due to operational complexity and "Out of Scope" constraint |
| Shared storage pool in metadata.yaml | Native Juju storage feature, well-supported | Manual symlinking across units - rejected due to fragility and manual intervention requirements |
| Detection before download | Reduces network usage, aligns with performance goals | Always download - rejected due to bandwidth waste and slower deployments |
| Per-unit worker directories | Maintains isolation, prevents state conflicts | Fully shared worker state - rejected due to concurrency risks and potential corruption |
